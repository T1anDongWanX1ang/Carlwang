#!/usr/bin/env python3
"""
æ¨ç‰¹å¼•ç”¨å…³ç³»æå–ä¸“ç”¨è„šæœ¬
ç”¨äºå¿«é€Ÿæµ‹è¯•å¼•ç”¨å…³ç³»åŠŸèƒ½
"""
import sys
import os
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.api.twitter_api import twitter_api
from src.database.tweet_dao import tweet_dao
from src.database.user_dao import user_dao
from src.database.quotation_dao import quotation_dao
from src.utils.data_mapper import data_mapper
from src.utils.quotation_extractor import quotation_extractor
from src.utils.logger import get_logger


class QuotationCrawler:
    """ä¸“é—¨ç”¨äºå¼•ç”¨å…³ç³»æå–çš„ç®€åŒ–çˆ¬è™«"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.logger = get_logger(__name__)
        self.api_client = twitter_api
        self.tweet_dao = tweet_dao
        self.user_dao = user_dao
        self.quotation_dao = quotation_dao
        self.data_mapper = data_mapper
        self.quotation_extractor = quotation_extractor
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.crawl_count = 0
        self.success_count = 0
        self.error_count = 0
        
        self.logger.info("ğŸ” å¼•ç”¨å…³ç³»æå–çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    def crawl_and_extract_quotations(self, list_id: str = None, max_pages: int = 5, 
                                   page_size: int = 20, hours_limit: int = 2) -> dict:
        """
        çˆ¬å–æ¨æ–‡å¹¶æå–å¼•ç”¨å…³ç³»
        
        Args:
            list_id: åˆ—è¡¨IDï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
            max_pages: æœ€å¤§é¡µæ•°
            page_size: æ¯é¡µå¤§å°
            hours_limit: æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            ç»“æœç»Ÿè®¡å­—å…¸
        """
        self.crawl_count += 1
        start_time = datetime.now()
        
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–å¹¶æå–å¼•ç”¨å…³ç³» (ç¬¬ {self.crawl_count} æ¬¡ï¼Œæ—¶é—´é™åˆ¶: {hours_limit}å°æ—¶)")
            
            # 1. ä»APIè·å–æ•°æ®
            api_data_list = self._fetch_api_data(list_id, max_pages, page_size, hours_limit)
            
            if not api_data_list:
                self.logger.warning("âŒ æœªè·å–åˆ°ä»»ä½•APIæ•°æ®")
                self.error_count += 1
                return self._get_result_stats(start_time, 0, 0, 0, 0)
            
            self.logger.info(f"ğŸ“¥ ä»APIè·å–åˆ° {len(api_data_list)} æ¡åŸå§‹æ•°æ®")
            
            # 2. æå–å¼•ç”¨å…³ç³»æ•°æ®
            self.logger.info("ğŸ”— å¼€å§‹æå–å¼•ç”¨å…³ç³»æ•°æ®...")
            quotations = self.quotation_extractor.extract_quotations_from_api_data(api_data_list)
            valid_quotations = self.quotation_extractor.filter_valid_quotations(quotations)
            
            quotation_count = len(valid_quotations)
            self.logger.info(f"âœ… å¼•ç”¨å…³ç³»æ•°æ®æå–å®Œæˆï¼Œè·å¾— {quotation_count} æ¡æœ‰æ•ˆå¼•ç”¨å…³ç³»")
            
            # 3. æ•°æ®æ˜ å°„å’Œè½¬æ¢ï¼ˆä»…ç”¨äºä¿å­˜æ¨æ–‡å’Œç”¨æˆ·æ•°æ®ï¼‰
            tweets = self._map_data_to_tweets(api_data_list)
            users = self._extract_users_from_api_data(api_data_list)
            
            self.logger.info(f"ğŸ“„ æˆåŠŸæ˜ å°„ {len(tweets)} æ¡æ¨æ–‡æ•°æ®ï¼Œ{len(users)} æ¡ç”¨æˆ·æ•°æ®")
            
            # 4. ä¿å­˜åˆ°æ•°æ®åº“
            # å…ˆä¿å­˜ç”¨æˆ·æ•°æ®
            user_saved_count = 0
            if users:
                user_saved_count = self._save_users_to_database(users)
                self.logger.info(f"ğŸ‘¥ æˆåŠŸä¿å­˜ {user_saved_count} æ¡ç”¨æˆ·æ•°æ®")
            
            # å†ä¿å­˜æ¨æ–‡æ•°æ®
            tweet_saved_count = self._save_tweets_to_database(tweets)
            self.logger.info(f"ğŸ“ æˆåŠŸä¿å­˜ {tweet_saved_count} æ¡æ¨æ–‡æ•°æ®")
            
            # æœ€åä¿å­˜å¼•ç”¨å…³ç³»æ•°æ®
            quotation_saved_count = 0
            if valid_quotations:
                quotation_saved_count = self._save_quotations_to_database(valid_quotations)
                self.logger.info(f"ğŸ”— æˆåŠŸä¿å­˜ {quotation_saved_count} æ¡å¼•ç”¨å…³ç³»æ•°æ®")
            
            # 5. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            stats = self.quotation_extractor.get_quotation_statistics(valid_quotations)
            if stats['total_quotations'] > 0:
                self.logger.info(f"ğŸ“Š å¼•ç”¨å…³ç³»ç»Ÿè®¡:")
                self.logger.info(f"   - æ€»å¼•ç”¨æ•°: {stats['total_quotations']}")
                self.logger.info(f"   - å¼•ç”¨ç”¨æˆ·æ•°: {stats['unique_quoters']}")
                self.logger.info(f"   - è¢«å¼•ç”¨ç”¨æˆ·æ•°: {stats['unique_quoted']}")
                
                if stats['top_quoters']:
                    self.logger.info(f"   - æœ€æ´»è·ƒå¼•ç”¨è€…: @{stats['top_quoters'][0]['user_name']} ({stats['top_quoters'][0]['quote_count']}æ¬¡)")
                
                if stats['top_quoted']:
                    self.logger.info(f"   - æœ€å¸¸è¢«å¼•ç”¨: @{stats['top_quoted'][0]['user_name']} ({stats['top_quoted'][0]['quoted_count']}æ¬¡)")
            
            self.success_count += 1
            return self._get_result_stats(start_time, len(api_data_list), tweet_saved_count, 
                                        user_saved_count, quotation_saved_count)
            
        except Exception as e:
            self.logger.error(f"âŒ çˆ¬å–å’Œæå–å¼•ç”¨å…³ç³»å¤±è´¥: {e}")
            self.error_count += 1
            return self._get_result_stats(start_time, 0, 0, 0, 0)
    
    def _fetch_api_data(self, list_id: str = None, max_pages: int = 5, 
                       page_size: int = 20, hours_limit: int = 2) -> list:
        """ä»APIè·å–æ•°æ®"""
        try:
            from src.utils.config_manager import config
            
            # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
            if not list_id:
                list_id = config.get('api', {}).get('default_params', {}).get('list_id')
            if not page_size:
                page_size = config.get('api', {}).get('pagination', {}).get('page_size', 20)
            
            self.logger.info(f"ğŸ“¡ æ­£åœ¨ä»APIè·å–æ•°æ®ï¼Œlist_id: {list_id}, é¡µæ•°é™åˆ¶: {max_pages}, æ—¶é—´é™åˆ¶: {hours_limit}å°æ—¶")
            
            # è°ƒç”¨APIè·å–æ¨æ–‡æ•°æ®
            api_data_generator = self.api_client.fetch_tweets_with_pagination(
                list_id=list_id,
                max_pages=max_pages,
                page_size=page_size,
                hours_limit=hours_limit
            )
            
            # å°†ç”Ÿæˆå™¨è½¬æ¢ä¸ºåˆ—è¡¨å¹¶å±•å¼€æ‰€æœ‰æ¨æ–‡
            api_data_list = []
            for page_data in api_data_generator:
                if isinstance(page_data, list):
                    api_data_list.extend(page_data)
                elif isinstance(page_data, dict):
                    api_data_list.append(page_data)
            
            # è·å–APIç»Ÿè®¡ä¿¡æ¯
            api_stats = self.api_client.get_request_stats()
            self.logger.info(f"ğŸ“ˆ APIè¯·æ±‚ç»Ÿè®¡: {api_stats}")
            
            return api_data_list
            
        except Exception as e:
            self.logger.error(f"âŒ APIæ•°æ®è·å–å¤±è´¥: {e}")
            return []
    
    def _map_data_to_tweets(self, api_data_list: list) -> list:
        """å°†APIæ•°æ®æ˜ å°„ä¸ºæ¨æ–‡å¯¹è±¡"""
        try:
            tweets = []
            for api_data in api_data_list:
                try:
                    tweet = self.data_mapper.map_api_data_to_tweet(api_data)
                    if tweet and tweet.validate():
                        tweets.append(tweet)
                except Exception as e:
                    self.logger.warning(f"æ¨æ–‡æ•°æ®æ˜ å°„å¤±è´¥: {e}")
                    continue
            return tweets
        except Exception as e:
            self.logger.error(f"æ‰¹é‡æ¨æ–‡æ•°æ®æ˜ å°„å¤±è´¥: {e}")
            return []
    
    def _extract_users_from_api_data(self, api_data_list: list) -> list:
        """ä»APIæ•°æ®ä¸­æå–ç”¨æˆ·æ•°æ®"""
        try:
            users = []
            seen_user_ids = set()
            
            for api_data in api_data_list:
                try:
                    user_data = api_data.get('user')
                    if user_data and isinstance(user_data, dict):
                        user_id = user_data.get('id_str')
                        if user_id and user_id not in seen_user_ids:
                            user = self.data_mapper.map_api_data_to_user(user_data)
                            if user and user.validate():
                                users.append(user)
                                seen_user_ids.add(user_id)
                    
                    # ä¹Ÿæå–è¢«å¼•ç”¨æ¨æ–‡çš„ç”¨æˆ·æ•°æ®
                    quoted_status = api_data.get('quoted_status')
                    if quoted_status and isinstance(quoted_status, dict):
                        quoted_user_data = quoted_status.get('user')
                        if quoted_user_data and isinstance(quoted_user_data, dict):
                            quoted_user_id = quoted_user_data.get('id_str')
                            if quoted_user_id and quoted_user_id not in seen_user_ids:
                                quoted_user = self.data_mapper.map_api_data_to_user(quoted_user_data)
                                if quoted_user and quoted_user.validate():
                                    users.append(quoted_user)
                                    seen_user_ids.add(quoted_user_id)
                                    
                except Exception as e:
                    self.logger.warning(f"ç”¨æˆ·æ•°æ®æå–å¤±è´¥: {e}")
                    continue
                    
            return users
        except Exception as e:
            self.logger.error(f"æ‰¹é‡ç”¨æˆ·æ•°æ®æå–å¤±è´¥: {e}")
            return []
    
    def _save_users_to_database(self, users: list) -> int:
        """ä¿å­˜ç”¨æˆ·æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            if not users:
                return 0
            return self.user_dao.batch_upsert_users(users)
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç”¨æˆ·æ•°æ®å¤±è´¥: {e}")
            return 0
    
    def _save_tweets_to_database(self, tweets: list) -> int:
        """ä¿å­˜æ¨æ–‡æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            if not tweets:
                return 0
            return self.tweet_dao.batch_upsert_tweets(tweets)
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ¨æ–‡æ•°æ®å¤±è´¥: {e}")
            return 0
    
    def _save_quotations_to_database(self, quotations: list) -> int:
        """ä¿å­˜å¼•ç”¨å…³ç³»åˆ°æ•°æ®åº“"""
        try:
            if not quotations:
                return 0
            
            # ç¡®ä¿æ•°æ®è¡¨å­˜åœ¨
            if not self.quotation_dao.create_table_if_not_exists():
                self.logger.error("åˆ›å»ºå¼•ç”¨å…³ç³»è¡¨å¤±è´¥")
                return 0
            
            return self.quotation_dao.batch_insert_quotations(quotations)
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¼•ç”¨å…³ç³»æ•°æ®å¤±è´¥: {e}")
            return 0
    
    def _get_result_stats(self, start_time, raw_count: int, tweet_count: int, 
                         user_count: int, quotation_count: int) -> dict:
        """ç”Ÿæˆç»“æœç»Ÿè®¡"""
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        return {
            'success': quotation_count > 0,
            'duration_seconds': round(duration, 2),
            'raw_data_count': raw_count,
            'tweets_saved': tweet_count,
            'users_saved': user_count,
            'quotations_saved': quotation_count,
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'crawl_count': self.crawl_count,
            'success_count': self.success_count,
            'error_count': self.error_count
        }
    
    def get_statistics(self) -> dict:
        """è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'crawl_count': self.crawl_count,
            'success_count': self.success_count,
            'error_count': self.error_count,
            'success_rate': (self.success_count / max(self.crawl_count, 1)) * 100,
            'api_stats': self.api_client.get_request_stats(),
            'database_tweet_count': self.tweet_dao.get_tweet_count(),
            'database_user_count': self.user_dao.get_user_count(),
            'database_quotation_count': self.quotation_dao.get_quotation_count()
        }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='æ¨ç‰¹å¼•ç”¨å…³ç³»æå–ä¸“ç”¨è„šæœ¬')
    parser.add_argument('--list-id', type=str, help='æ¨ç‰¹åˆ—è¡¨ID')
    parser.add_argument('--max-pages', type=int, default=5, help='æœ€å¤§é¡µæ•° (é»˜è®¤: 5)')
    parser.add_argument('--page-size', type=int, default=20, help='æ¯é¡µå¤§å° (é»˜è®¤: 20)')
    parser.add_argument('--hours-limit', type=int, default=2, help='æ—¶é—´é™åˆ¶å°æ—¶æ•° (é»˜è®¤: 2)')
    parser.add_argument('--quiet', '-q', action='store_true', help='é™é»˜æ¨¡å¼')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.quiet:
        logging.getLogger().setLevel(logging.WARNING)
    
    try:
        print("ğŸ” æ¨ç‰¹å¼•ç”¨å…³ç³»æå–è„šæœ¬")
        print("=" * 50)
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = QuotationCrawler()
        
        # æ‰§è¡Œçˆ¬å–å’Œæå–
        result = crawler.crawl_and_extract_quotations(
            list_id=args.list_id,
            max_pages=args.max_pages,
            page_size=args.page_size,
            hours_limit=args.hours_limit
        )
        
        # è¾“å‡ºç»“æœ
        print("\nğŸ“Š æ‰§è¡Œç»“æœ:")
        print(f"   âœ… æˆåŠŸ: {result['success']}")
        print(f"   â±ï¸  è€—æ—¶: {result['duration_seconds']} ç§’")
        print(f"   ğŸ“¥ åŸå§‹æ•°æ®: {result['raw_data_count']} æ¡")
        print(f"   ğŸ“ æ¨æ–‡ä¿å­˜: {result['tweets_saved']} æ¡")
        print(f"   ğŸ‘¥ ç”¨æˆ·ä¿å­˜: {result['users_saved']} æ¡") 
        print(f"   ğŸ”— å¼•ç”¨å…³ç³»: {result['quotations_saved']} æ¡")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = crawler.get_statistics()
        print(f"\nğŸ“ˆ æ•°æ®åº“ç»Ÿè®¡:")
        print(f"   ğŸ“ æ€»æ¨æ–‡æ•°: {stats['database_tweet_count']}")
        print(f"   ğŸ‘¥ æ€»ç”¨æˆ·æ•°: {stats['database_user_count']}")
        print(f"   ğŸ”— æ€»å¼•ç”¨å…³ç³»æ•°: {stats['database_quotation_count']}")
        
        # è¾“å‡ºæˆåŠŸç‡
        print(f"\nğŸ¯ çˆ¬è™«ç»Ÿè®¡:")
        print(f"   æ€»æ‰§è¡Œæ¬¡æ•°: {stats['crawl_count']}")
        print(f"   æˆåŠŸæ¬¡æ•°: {stats['success_count']}")
        print(f"   å¤±è´¥æ¬¡æ•°: {stats['error_count']}")
        print(f"   æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        
        return 0 if result['success'] else 1
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        return 130
    except Exception as e:
        print(f"\nâŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)