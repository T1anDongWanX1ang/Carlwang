"""
Twitteræ•°æ®çˆ¬è™«æ ¸å¿ƒæ¨¡å—
"""
import logging
import os
from typing import List, Dict, Any, Optional
from datetime import datetime

# åç«¯å¯åˆ‡æ¢ï¼šé»˜è®¤ä½¿ç”¨ Twitter API (twitterapi)ï¼Œè®¾ç½®ç¯å¢ƒå˜é‡ TWITTER_API_BACKEND=tweetscout ä½¿ç”¨ TweetScout
# ä¸ºäº†å‘åå…¼å®¹KOLæ¨æ–‡çˆ¬å–ï¼Œç°åœ¨é»˜è®¤ä½¿ç”¨ twitterapi
if os.getenv("TWITTER_API_BACKEND", "twitterapi").lower() == "tweetscout":
    from .api.twitter_api import twitter_api as selected_api_client
else:
    from .api.twitter_api_twitterapi import twitter_api as selected_api_client
from .database.tweet_dao import tweet_dao
from .database.user_dao import user_dao
from .database.quotation_dao import quotation_dao
from .utils.data_mapper import data_mapper
from .utils.config_manager import config
from .utils.logger import get_logger
from .utils.tweet_enricher import tweet_enricher
from .utils.simple_tweet_enricher import simple_tweet_enricher
from .utils.quotation_extractor import quotation_extractor
# from .utils.user_language_integration import UserLanguageIntegration  # è¯­è¨€æ£€æµ‹å·²ç¦ç”¨
from .models.tweet import Tweet
from .models.user import TwitterUser
# from .topic_engine import topic_engine  # è¯é¢˜åˆ†æå·²ç§»é™¤
# from .kol_engine import kol_engine  # KOLåˆ†æå·²ç¦ç”¨
from .project_engine import project_engine


class TwitterCrawler:
    """Twitteræ•°æ®çˆ¬è™«"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.logger = get_logger(__name__)
        self.api_client = selected_api_client
        self.tweet_dao = tweet_dao
        self.user_dao = user_dao
        self.quotation_dao = quotation_dao
        self.data_mapper = data_mapper
        # self.topic_engine = topic_engine  # è¯é¢˜åˆ†æå·²ç§»é™¤
        # self.kol_engine = kol_engine  # KOLåˆ†æå·²ç¦ç”¨
        self.project_engine = project_engine
        self.tweet_enricher = tweet_enricher
        self.quotation_extractor = quotation_extractor

        # åˆå§‹åŒ–ç”¨æˆ·è¯­è¨€é›†æˆå™¨ï¼ˆå·²ç¦ç”¨ï¼‰
        # from .api.chatgpt_client import chatgpt_client
        # self.user_language_integration = UserLanguageIntegration(
        #     db_manager=self.user_dao.db_manager,
        #     chatgpt_client=chatgpt_client
        # )

        # çˆ¬å–ç»Ÿè®¡
        self.crawl_count = 0
        self.success_count = 0
        self.error_count = 0
        self.last_crawl_time = None
        
        self.logger.info("Twitterçˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    def crawl_tweets(self, list_id: str = None, list_ids: List[str] = None, max_pages: int = None, 
                    page_size: int = None, hours_limit: int = 2) -> bool:
        """
        çˆ¬å–æ¨æ–‡æ•°æ®
        
        Args:
            list_id: å•ä¸ªåˆ—è¡¨IDï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼ï¼ˆå‘åå…¼å®¹ï¼‰
            list_ids: å¤šä¸ªåˆ—è¡¨IDåˆ—è¡¨ï¼Œä¼˜å…ˆçº§é«˜äºlist_id
            max_pages: æœ€å¤§é¡µæ•°ï¼ˆä¸è¶…è¿‡15é¡µï¼‰
            page_size: æ¯é¡µå¤§å°
            hours_limit: æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰ï¼Œåªæ‹‰å–è¿‡å»Nå°æ—¶çš„æ¨æ–‡ï¼Œé»˜è®¤2å°æ—¶ï¼ˆç”Ÿäº§ç¯å¢ƒä½¿ç”¨UTCæ—¶é—´ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        self.crawl_count += 1
        self.last_crawl_time = datetime.now()
        
        try:
            self.logger.info(f"å¼€å§‹çˆ¬å–æ¨æ–‡æ•°æ® (ç¬¬ {self.crawl_count} æ¬¡ï¼Œæ—¶é—´é™åˆ¶: {hours_limit}å°æ—¶)")
            
            # 1. ä»APIè·å–æ•°æ®ï¼ˆæ”¯æŒå¤šä¸ªlistå¹¶è¡Œå¤„ç†ï¼Œæœ€å¤š15é¡µï¼Œåªæ‹‰å–è¿‡å»8å°æ—¶ï¼‰
            api_data_list = self._fetch_api_data(list_id, list_ids, max_pages, page_size, hours_limit)
            
            if not api_data_list:
                self.logger.warning("æœªè·å–åˆ°ä»»ä½•APIæ•°æ®")
                self.error_count += 1
                return False
            
            self.logger.info(f"ä»APIè·å–åˆ° {len(api_data_list)} æ¡åŸå§‹æ•°æ®")
            
            # 2. æ•°æ®æ˜ å°„å’Œè½¬æ¢
            tweets = self._map_data_to_tweets(api_data_list)
            
            if not tweets:
                self.logger.warning("æ•°æ®æ˜ å°„åæ²¡æœ‰æœ‰æ•ˆçš„æ¨æ–‡æ•°æ®")
                self.error_count += 1
                return False
            
            self.logger.info(f"æˆåŠŸæ˜ å°„ {len(tweets)} æ¡æ¨æ–‡æ•°æ®")
            
            # 2.1 æå–ç”¨æˆ·æ•°æ®
            users = self._extract_users_from_api_data(api_data_list)
            self.logger.info(f"æˆåŠŸæå– {len(users)} æ¡ç”¨æˆ·æ•°æ®")
            
            # 2.2 æ„å»ºç”¨æˆ·æ•°æ®æ˜ å°„ï¼ˆç”¨äºæ¨æ–‡å¢å¼ºï¼‰
            # å»ºç«‹ tweet_id -> user_data çš„æ˜ å°„å…³ç³»
            user_data_map = {}
            for api_data in api_data_list:
                try:
                    tweet_id = api_data.get('id_str')
                    # å…¼å®¹ä¸¤ç§APIï¼šTweetScoutä½¿ç”¨'user'ï¼ŒTwitter APIä½¿ç”¨'author'
                    user_data = api_data.get('user') or api_data.get('author')
                    if tweet_id and user_data and isinstance(user_data, dict):
                        user_data_map[tweet_id] = user_data
                except Exception as e:
                    self.logger.warning(f"æ„å»ºç”¨æˆ·æ•°æ®æ˜ å°„å¤±è´¥: {e}")
                    continue
            
            # 2.3 æå–å¼•ç”¨å…³ç³»æ•°æ®
            self.logger.info("å¼€å§‹æå–å¼•ç”¨å…³ç³»æ•°æ®...")
            quotations = self.quotation_extractor.extract_quotations_from_api_data(api_data_list)
            valid_quotations = self.quotation_extractor.filter_valid_quotations(quotations)
            self.logger.info(f"å¼•ç”¨å…³ç³»æ•°æ®æå–å®Œæˆï¼Œè·å¾— {len(valid_quotations)} æ¡æœ‰æ•ˆå¼•ç”¨å…³ç³»")
            
            # 2.4 å¢å¼ºæ¨æ–‡æ•°æ®ï¼ˆæ·»åŠ  kol_id å’Œ entity_idï¼‰
            self.logger.info("å¼€å§‹å¢å¼ºæ¨æ–‡æ•°æ®...")
            enriched_tweets = self.tweet_enricher.enrich_tweets(tweets, user_data_map)
            self.logger.info(f"æ¨æ–‡å¢å¼ºå®Œæˆï¼Œå¤„ç†äº† {len(enriched_tweets)} æ¡æ¨æ–‡")
            
            # 3. å­˜å‚¨åˆ°æ•°æ®åº“
            # å…ˆä¿å­˜ç”¨æˆ·æ•°æ®ï¼ˆå› ä¸ºæ¨æ–‡å¯èƒ½å¼•ç”¨ç”¨æˆ·ï¼‰
            if users:
                user_saved_count = self._save_users_to_database(users)
                self.logger.info(f"æˆåŠŸä¿å­˜ {user_saved_count} æ¡ç”¨æˆ·æ•°æ®")
            
            # ç„¶åä¿å­˜å¢å¼ºåçš„æ¨æ–‡æ•°æ®
            tweet_saved_count = self._save_tweets_to_database(enriched_tweets)
            
            # ä¿å­˜å¼•ç”¨å…³ç³»æ•°æ®
            quotation_saved_count = 0
            if valid_quotations:
                quotation_saved_count = self._save_quotations_to_database(valid_quotations)
                self.logger.info(f"æˆåŠŸä¿å­˜ {quotation_saved_count} æ¡å¼•ç”¨å…³ç³»æ•°æ®")
            
            # 4. æ•°æ®ä¿å­˜å®Œæˆ
            if tweet_saved_count > 0:
                self.logger.info(f"æˆåŠŸä¿å­˜ {tweet_saved_count} æ¡æ¨æ–‡åˆ°æ•°æ®åº“")
                
                # è¯é¢˜åˆ†æå·²ç§»é™¤ - åœ¨å…¶ä»–ç‹¬ç«‹è„šæœ¬ä¸­å¤„ç†
                # try:
                #     self.logger.info("å¼€å§‹è¿›è¡Œè¯é¢˜åˆ†æ...")
                #     topic_success = self.topic_engine.analyze_recent_tweets(hours=1, max_tweets=50)
                #     
                #     if topic_success:
                #         self.logger.info("è¯é¢˜åˆ†æå®Œæˆ")
                #     else:
                #         self.logger.warning("è¯é¢˜åˆ†æå¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»æµç¨‹")
                #         
                # except Exception as e:
                #     self.logger.error(f"è¯é¢˜åˆ†æå¼‚å¸¸: {e}")
                
                # KOLåˆ†æå·²ç¦ç”¨
                # try:
                #     self.logger.info("å¼€å§‹è¿›è¡ŒKOLåˆ†æ...")
                #     kol_success = self.kol_engine.analyze_all_users_as_kols(min_followers=50000, max_users=10)
                #     
                #     if kol_success:
                #         self.logger.info("KOLåˆ†æå®Œæˆ")
                #     else:
                #         self.logger.warning("KOLåˆ†æå¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»æµç¨‹")
                #         
                # except Exception as e:
                #     self.logger.error(f"KOLåˆ†æå¼‚å¸¸: {e}")
                
                # è¿›è¡Œé¡¹ç›®åˆ†æ
                try:
                    self.logger.info("å¼€å§‹è¿›è¡Œé¡¹ç›®åˆ†æ...")
                    project_success = self.project_engine.analyze_recent_tweets(hours=1, max_tweets=50)

                    if project_success:
                        self.logger.info("é¡¹ç›®åˆ†æå®Œæˆ")
                    else:
                        self.logger.warning("é¡¹ç›®åˆ†æå¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»æµç¨‹")

                except Exception as e:
                    self.logger.error(f"é¡¹ç›®åˆ†æå¼‚å¸¸: {e}")

                # è¿›è¡Œæ´»åŠ¨æ£€æµ‹å’Œç»“æ„åŒ–
                try:
                    self.logger.info("å¼€å§‹è¿›è¡Œæ´»åŠ¨æ£€æµ‹å’Œç»“æ„åŒ–...")
                    activity_success = self._detect_and_structure_activities(enriched_tweets)

                    if activity_success:
                        self.logger.info("æ´»åŠ¨æ£€æµ‹å’Œç»“æ„åŒ–å®Œæˆ")
                    else:
                        self.logger.warning("æ´»åŠ¨æ£€æµ‹å’Œç»“æ„åŒ–å¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»æµç¨‹")

                except Exception as e:
                    self.logger.error(f"æ´»åŠ¨æ£€æµ‹å¼‚å¸¸: {e}")

                self.success_count += 1
                return True
            else:
                self.logger.error("ä¿å­˜æ¨æ–‡åˆ°æ•°æ®åº“å¤±è´¥")
                self.error_count += 1
                return False
                
        except Exception as e:
            self.logger.error(f"çˆ¬å–æ¨æ–‡æ•°æ®å¼‚å¸¸: {e}")
            self.error_count += 1
            return False
    
    def _fetch_api_data(self, list_id: str = None, list_ids: List[str] = None, max_pages: int = None, 
                       page_size: int = None, hours_limit: int = 2) -> List[Dict[str, Any]]:
        """
        ä»APIè·å–æ•°æ®ï¼ˆæ”¯æŒå¹¶è¡Œè·å–å¤šä¸ªlistï¼‰
        
        Args:
            list_id: å•ä¸ªåˆ—è¡¨IDï¼ˆå‘åå…¼å®¹ï¼‰
            list_ids: å¤šä¸ªåˆ—è¡¨IDåˆ—è¡¨ï¼Œä¼˜å…ˆçº§é«˜äºlist_id
            max_pages: æœ€å¤§é¡µæ•°ï¼ˆä¸è¶…è¿‡15é¡µï¼‰
            page_size: æ¯é¡µå¤§å°
            hours_limit: æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰ï¼Œåªæ‹‰å–è¿‡å»Nå°æ—¶çš„æ¨æ–‡
            
        Returns:
            APIæ•°æ®åˆ—è¡¨
        """
        try:
            # ç¡®å®šè¦ä½¿ç”¨çš„list_idsåˆ—è¡¨
            if list_ids is not None:
                # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„list_ids
                target_list_ids = list_ids
            elif list_id is not None:
                # å‘åå…¼å®¹ï¼šå¦‚æœåªä¼ å…¥äº†å•ä¸ªlist_idï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                target_list_ids = [list_id]
            else:
                # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
                target_list_ids = config.get('api.default_params.list_ids', [config.get('api.default_params.list_id')])
            
            self.logger.info(f"æ­£åœ¨å¹¶è¡Œè·å– {len(target_list_ids)} ä¸ªlistçš„æ•°æ®: {target_list_ids}, æ—¶é—´é™åˆ¶: {hours_limit}å°æ—¶")
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªlistï¼Œä½¿ç”¨åŸæœ‰çš„ä¸²è¡Œé€»è¾‘
            if len(target_list_ids) == 1:
                list_id_single = target_list_ids[0]
                self.logger.info(f"å•ä¸ªlistæ¨¡å¼ï¼Œlist_id: {list_id_single}")
                
                api_data_list = self.api_client.fetch_all_tweets(
                    list_id=list_id_single,
                    max_pages=max_pages,
                    page_size=page_size,
                    hours_limit=hours_limit
                )
                
                # è·å–APIè¯·æ±‚ç»Ÿè®¡
                stats = self.api_client.get_request_stats()
                self.logger.info(f"APIè¯·æ±‚ç»Ÿè®¡: {stats}")
                
                return api_data_list
            
            # å¤šä¸ªlistçš„å¹¶è¡Œå¤„ç†é€»è¾‘
            import concurrent.futures
            
            all_api_data = []
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè·å–å¤šä¸ªlistçš„æ•°æ®
            with concurrent.futures.ThreadPoolExecutor(max_workers=len(target_list_ids)) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_list_id = {
                    executor.submit(
                        self.api_client.fetch_all_tweets,
                        list_id=single_list_id,
                        max_pages=max_pages,
                        page_size=page_size,
                        hours_limit=hours_limit
                    ): single_list_id for single_list_id in target_list_ids
                }
                
                # æ”¶é›†ç»“æœ
                for future in concurrent.futures.as_completed(future_to_list_id):
                    single_list_id = future_to_list_id[future]
                    try:
                        api_data = future.result()
                        if api_data:
                            self.logger.info(f"list_id {single_list_id} è·å–åˆ° {len(api_data)} æ¡æ•°æ®")
                            all_api_data.extend(api_data)
                        else:
                            self.logger.warning(f"list_id {single_list_id} æœªè·å–åˆ°æ•°æ®")
                    except Exception as e:
                        self.logger.error(f"è·å– list_id {single_list_id} æ•°æ®å¤±è´¥: {e}")
            
            # è·å–APIè¯·æ±‚ç»Ÿè®¡
            stats = self.api_client.get_request_stats()
            self.logger.info(f"å¹¶è¡ŒAPIè¯·æ±‚ç»Ÿè®¡: {stats}")
            self.logger.info(f"æ€»å…±è·å–åˆ° {len(all_api_data)} æ¡APIæ•°æ®")
            
            return all_api_data
            
        except Exception as e:
            self.logger.error(f"ä»APIè·å–æ•°æ®å¤±è´¥: {e}")
            return []
    
    def _map_data_to_tweets(self, api_data_list: List[Dict[str, Any]]) -> List[Tweet]:
        """
        å°†APIæ•°æ®æ˜ å°„ä¸ºTweetå¯¹è±¡
        
        Args:
            api_data_list: APIæ•°æ®åˆ—è¡¨
            
        Returns:
            Tweetå¯¹è±¡åˆ—è¡¨
        """
        try:
            self.logger.info("å¼€å§‹æ˜ å°„APIæ•°æ®åˆ°Tweetå¯¹è±¡...")
            
            tweets = self.data_mapper.map_api_data_list_to_tweets(api_data_list)
            
            # æ•°æ®å»é‡ï¼ˆåŸºäºid_strï¼‰
            unique_tweets = self._deduplicate_tweets(tweets)
            
            if len(unique_tweets) != len(tweets):
                self.logger.info(f"å»é‡åå‰©ä½™ {len(unique_tweets)} æ¡æ¨æ–‡ï¼ˆåŸ {len(tweets)} æ¡ï¼‰")
            
            return unique_tweets
            
        except Exception as e:
            self.logger.error(f"æ•°æ®æ˜ å°„å¤±è´¥: {e}")
            return []
    
    def _deduplicate_tweets(self, tweets: List[Tweet]) -> List[Tweet]:
        """
        æ¨æ–‡å»é‡
        
        Args:
            tweets: æ¨æ–‡åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ¨æ–‡åˆ—è¡¨
        """
        seen_ids = set()
        unique_tweets = []
        
        for tweet in tweets:
            if tweet.id_str not in seen_ids:
                seen_ids.add(tweet.id_str)
                unique_tweets.append(tweet)
        
        return unique_tweets
    
    def _extract_users_from_api_data(self, api_data_list: List[Dict[str, Any]]) -> List[TwitterUser]:
        """
        ä»APIæ•°æ®ä¸­æå–ç”¨æˆ·ä¿¡æ¯
        
        Args:
            api_data_list: APIæ•°æ®åˆ—è¡¨
            
        Returns:
            ç”¨æˆ·å¯¹è±¡åˆ—è¡¨
        """
        try:
            self.logger.info("å¼€å§‹ä»APIæ•°æ®ä¸­æå–ç”¨æˆ·ä¿¡æ¯...")
            
            users = self.data_mapper.extract_users_from_tweets(api_data_list)
            
            # ç”¨æˆ·å»é‡ï¼ˆåŸºäºid_strï¼‰
            unique_users = self._deduplicate_users(users)
            
            if len(unique_users) != len(users):
                self.logger.info(f"ç”¨æˆ·å»é‡åå‰©ä½™ {len(unique_users)} ä¸ªç”¨æˆ·ï¼ˆåŸ {len(users)} ä¸ªï¼‰")
            
            return unique_users
            
        except Exception as e:
            self.logger.error(f"æå–ç”¨æˆ·æ•°æ®å¤±è´¥: {e}")
            return []
    
    def _deduplicate_users(self, users: List[TwitterUser]) -> List[TwitterUser]:
        """
        ç”¨æˆ·å»é‡
        
        Args:
            users: ç”¨æˆ·åˆ—è¡¨
            
        Returns:
            å»é‡åçš„ç”¨æˆ·åˆ—è¡¨
        """
        seen_ids = set()
        unique_users = []
        
        for user in users:
            if user.id_str not in seen_ids:
                seen_ids.add(user.id_str)
                unique_users.append(user)
        
        return unique_users
    
    def _extract_user_data_from_api(self, api_data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä»APIæ•°æ®ä¸­æå–åŸå§‹ç”¨æˆ·æ•°æ®ï¼ˆç”¨äºæ¨æ–‡å¢å¼ºï¼‰
        
        Args:
            api_data_list: APIåŸå§‹æ•°æ®åˆ—è¡¨
            
        Returns:
            åŸå§‹ç”¨æˆ·æ•°æ®åˆ—è¡¨
        """
        user_data_list = []
        
        for api_data in api_data_list:
            try:
                # ä»tweetæ•°æ®ä¸­æå–userä¿¡æ¯
                user_data = api_data.get('user')
                if user_data and isinstance(user_data, dict):
                    user_data_list.append(user_data)
                    
            except Exception as e:
                self.logger.warning(f"æå–ç”¨æˆ·æ•°æ®å¤±è´¥: {e}")
                continue
        
        return user_data_list
    
    def _save_users_to_database(self, users: List[TwitterUser]) -> int:
        """
        ä¿å­˜ç”¨æˆ·åˆ°æ•°æ®åº“

        Args:
            users: ç”¨æˆ·åˆ—è¡¨

        Returns:
            æˆåŠŸä¿å­˜çš„æ•°é‡
        """
        try:
            self.logger.info(f"å¼€å§‹ä¿å­˜ {len(users)} æ¡ç”¨æˆ·åˆ°æ•°æ®åº“...")

            # è¯­è¨€æ£€æµ‹å·²ç¦ç”¨ï¼Œç›´æ¥æ‰¹é‡ä¿å­˜ç”¨æˆ·æ•°æ®
            # enhanced_users = self._enhance_users_with_language(users)
            saved_count = self.user_dao.batch_upsert_users(users)

            return saved_count

        except Exception as e:
            self.logger.error(f"ä¿å­˜ç”¨æˆ·åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return 0
    
    # def _enhance_users_with_language(self, users: List[TwitterUser]) -> List[TwitterUser]:
    #     """
    #     ä¸ºç”¨æˆ·åˆ—è¡¨æ·»åŠ è¯­è¨€æ£€æµ‹ä¿¡æ¯ï¼ˆå·²ç¦ç”¨ï¼‰
    #
    #     Args:
    #         users: ç”¨æˆ·åˆ—è¡¨
    #
    #     Returns:
    #         å¢å¼ºåçš„ç”¨æˆ·åˆ—è¡¨
    #     """
    #     try:
    #         self.logger.info(f"å¼€å§‹ä¸º {len(users)} ä¸ªç”¨æˆ·è¿›è¡Œè¯­è¨€æ£€æµ‹...")
    #
    #         # ä½¿ç”¨æ‰¹é‡è¯­è¨€æ£€æµ‹
    #         enhanced_users = self.user_language_integration.enhance_users_batch(
    #             users=users,
    #             use_ai_fallback=False  # æš‚æ—¶ä¸ä½¿ç”¨AIè¾…åŠ©ï¼Œé¿å…è¿‡å¤šAPIè°ƒç”¨
    #         )
    #
    #         self.logger.info(f"å®Œæˆè¯­è¨€æ£€æµ‹ï¼Œå¢å¼ºäº† {len(enhanced_users)} ä¸ªç”¨æˆ·")
    #         return enhanced_users
    #
    #     except Exception as e:
    #         self.logger.error(f"ç”¨æˆ·è¯­è¨€æ£€æµ‹å¤±è´¥: {e}")
    #         # å¦‚æœè¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œè®¾ç½®é»˜è®¤è¯­è¨€å¹¶è¿”å›åŸç”¨æˆ·åˆ—è¡¨
    #         for user in users:
    #             if not hasattr(user, 'language') or user.language is None:
    #                 user.language = "English"  # é»˜è®¤è®¾ä¸ºEnglish
    #         return users
    
    def _save_tweets_to_database(self, tweets: List[Tweet]) -> int:
        """
        ä¿å­˜æ¨æ–‡åˆ°æ•°æ®åº“
        
        Args:
            tweets: æ¨æ–‡åˆ—è¡¨
            
        Returns:
            æˆåŠŸä¿å­˜çš„æ•°é‡
        """
        try:
            self.logger.info(f"å¼€å§‹ä¿å­˜ {len(tweets)} æ¡æ¨æ–‡åˆ°æ•°æ®åº“...")
            
            # æ‰¹é‡ä¿å­˜
            saved_count = self.tweet_dao.batch_upsert_tweets(tweets)
            
            return saved_count
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ¨æ–‡åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return 0
    
    def _save_quotations_to_database(self, quotations: List[Dict[str, Any]]) -> int:
        """
        ä¿å­˜å¼•ç”¨å…³ç³»åˆ°æ•°æ®åº“
        
        Args:
            quotations: å¼•ç”¨å…³ç³»æ•°æ®åˆ—è¡¨
            
        Returns:
            æˆåŠŸä¿å­˜çš„æ•°é‡
        """
        try:
            if not quotations:
                self.logger.info("æ²¡æœ‰å¼•ç”¨å…³ç³»æ•°æ®éœ€è¦ä¿å­˜")
                return 0
            
            # ç¡®ä¿æ•°æ®è¡¨å­˜åœ¨
            if not self.quotation_dao.create_table_if_not_exists():
                self.logger.error("åˆ›å»ºå¼•ç”¨å…³ç³»è¡¨å¤±è´¥")
                return 0
            
            self.logger.info(f"å¼€å§‹ä¿å­˜ {len(quotations)} æ¡å¼•ç”¨å…³ç³»åˆ°æ•°æ®åº“...")
            
            # æ‰¹é‡ä¿å­˜
            saved_count = self.quotation_dao.batch_insert_quotations(quotations)
            
            return saved_count
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¼•ç”¨å…³ç³»åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return 0

    def _detect_and_structure_activities(self, tweets: List[Tweet]) -> bool:
        """
        æ£€æµ‹æ¨æ–‡ä¸­çš„æ´»åŠ¨å¹¶ç»“æ„åŒ–å­˜å‚¨

        Args:
            tweets: æ¨æ–‡åˆ—è¡¨

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            from .api.chatgpt_client import chatgpt_client
            import json

            # æ´»åŠ¨å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºåˆæ­¥è¿‡æ»¤ï¼‰
            activity_keywords = [
                'campaign', 'airdrop', 'quest', 'reward', 'giveaway',
                'bounty', 'contest', 'prize', 'distribution', 'incentive',
                'ç©ºæŠ•', 'æ´»åŠ¨', 'å¥–åŠ±', 'èµ é€'
            ]

            # è¿‡æ»¤åŒ…å«æ´»åŠ¨å…³é”®è¯çš„æ¨æ–‡
            candidate_tweets = []
            for tweet in tweets:
                if tweet.full_text:
                    text_lower = tweet.full_text.lower()
                    if any(keyword in text_lower for keyword in activity_keywords):
                        candidate_tweets.append(tweet)

            self.logger.info(f"ä» {len(tweets)} æ¡æ¨æ–‡ä¸­ç­›é€‰å‡º {len(candidate_tweets)} æ¡å€™é€‰æ´»åŠ¨æ¨æ–‡")

            if not candidate_tweets:
                self.logger.info("æ²¡æœ‰å‘ç°åŒ…å«æ´»åŠ¨å…³é”®è¯çš„æ¨æ–‡")
                return True

            # æ£€æµ‹å’Œç»“æ„åŒ–æ´»åŠ¨æ•°æ®
            activity_count = 0
            for tweet in candidate_tweets:
                try:
                    # 1. ä½¿ç”¨AIæ£€æµ‹æ˜¯å¦ä¸ºçœŸæ­£çš„æ´»åŠ¨
                    is_activity = chatgpt_client.detect_campaign_announcement([tweet.full_text])

                    if is_activity:
                        self.logger.info(f"æ£€æµ‹åˆ°æ´»åŠ¨æ¨æ–‡: {tweet.id_str}")

                        # 2. ç”Ÿæˆæ¨æ–‡URLï¼ˆä½¿ç”¨é€šç”¨æ ¼å¼ï¼Œä¸éœ€è¦ç”¨æˆ·åï¼‰
                        tweet_url = f"https://twitter.com/i/status/{tweet.id_str}"

                        # 3. æå–ç»“æ„åŒ–æ•°æ®
                        activity_data = chatgpt_client.extract_activity_structured_data(
                            tweet_text=tweet.full_text,
                            tweet_url=tweet_url,
                            tweet_time=str(tweet.created_at) if tweet.created_at else ""
                        )

                        if activity_data:
                            # 4. å°†ç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å­˜å‚¨åˆ°activity_detailå­—æ®µ
                            activity_detail_json = json.dumps(activity_data, ensure_ascii=False)

                            # 5. æ›´æ–°æ•°æ®åº“
                            success = self._update_tweet_activity_status(
                                tweet_id=tweet.id_str,
                                is_activity=1,
                                activity_detail=activity_detail_json
                            )

                            if success:
                                activity_count += 1
                                self.logger.info(f"æ´»åŠ¨æ•°æ®å·²ç»“æ„åŒ–: {activity_data['title']}")
                        else:
                            self.logger.warning(f"æ— æ³•æå–æ´»åŠ¨ç»“æ„åŒ–æ•°æ®: {tweet.id_str}")

                except Exception as e:
                    self.logger.error(f"å¤„ç†æ¨æ–‡æ´»åŠ¨æ£€æµ‹å¤±è´¥ {tweet.id_str}: {e}")
                    continue

            self.logger.info(f"æˆåŠŸæ£€æµ‹å¹¶ç»“æ„åŒ– {activity_count} æ¡æ´»åŠ¨æ¨æ–‡")
            return True

        except Exception as e:
            self.logger.error(f"æ´»åŠ¨æ£€æµ‹å’Œç»“æ„åŒ–å¤±è´¥: {e}")
            return False

    def _update_tweet_activity_status(self, tweet_id: str, is_activity: int,
                                     activity_detail: str) -> bool:
        """
        æ›´æ–°æ¨æ–‡çš„æ´»åŠ¨çŠ¶æ€

        Args:
            tweet_id: æ¨æ–‡ID
            is_activity: æ˜¯å¦ä¸ºæ´»åŠ¨æ¨æ–‡ï¼ˆ0æˆ–1ï¼‰
            activity_detail: æ´»åŠ¨è¯¦æƒ…ï¼ˆJSONå­—ç¬¦ä¸²ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            table_name = self.tweet_dao.db_manager.db_config.get('tables', {}).get('tweet', 'twitter_tweet')

            sql = f"""
            UPDATE {table_name}
            SET is_activity = %s, activity_detail = %s
            WHERE id_str = %s
            """

            affected_rows = self.tweet_dao.db_manager.execute_update(
                sql, (is_activity, activity_detail, tweet_id)
            )

            return affected_rows > 0

        except Exception as e:
            self.logger.error(f"æ›´æ–°æ¨æ–‡æ´»åŠ¨çŠ¶æ€å¤±è´¥ {tweet_id}: {e}")
            return False

    def test_connection(self) -> bool:
        """
        æµ‹è¯•æ•°æ®åº“è¿æ¥
        
        Returns:
            è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info("æµ‹è¯•æ•°æ®åº“è¿æ¥...")
            success = self.tweet_dao.db_manager.test_connection()
            
            if success:
                self.logger.info("æ•°æ®åº“è¿æ¥æµ‹è¯•æˆåŠŸ")
            else:
                self.logger.error("æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥")
            
            return success
            
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_api_connection(self) -> bool:
        """
        æµ‹è¯•APIè¿æ¥
        
        Returns:
            APIè¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info("æµ‹è¯•APIè¿æ¥...")
            
            # å°è¯•è·å–å°‘é‡æ•°æ®
            test_data, _ = self.api_client.fetch_tweets(count=1)
            
            success = test_data is not None and len(test_data) >= 0
            
            if success:
                self.logger.info("APIè¿æ¥æµ‹è¯•æˆåŠŸ")
            else:
                self.logger.error("APIè¿æ¥æµ‹è¯•å¤±è´¥")
            
            return success
            
        except Exception as e:
            self.logger.error(f"APIè¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return {
            'crawl_count': self.crawl_count,
            'success_count': self.success_count,
            'error_count': self.error_count,
            'success_rate': (self.success_count / max(self.crawl_count, 1)) * 100,
            'last_crawl_time': self.last_crawl_time.isoformat() if self.last_crawl_time else None,
            'api_stats': self.api_client.get_request_stats(),
            'database_tweet_count': self.tweet_dao.get_tweet_count(),
            'database_user_count': self.user_dao.get_user_count(),
            'database_quotation_count': self.quotation_dao.get_quotation_count(),
            # 'topic_stats': self.topic_engine.get_topic_statistics(),  # è¯é¢˜åˆ†æå·²ç§»é™¤
            # 'kol_stats': self.kol_engine.get_kol_statistics(),  # KOLåˆ†æå·²ç¦ç”¨
            'project_stats': self.project_engine.get_project_statistics()
        }
    
    def reset_statistics(self) -> None:
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.crawl_count = 0
        self.success_count = 0
        self.error_count = 0
        self.last_crawl_time = None
        self.api_client.reset_stats()
        self.logger.info("çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
    
    def crawl_project_tweets(self, max_pages: int = None, page_size: int = None, hours_limit: int = 2) -> bool:
        """
        çˆ¬å–é¡¹ç›®æ¨æ–‡æ•°æ®ï¼ˆç®€åŒ–æµç¨‹ç‰ˆæœ¬ï¼‰
        
        Args:
            max_pages: æœ€å¤§é¡µæ•°ï¼ˆä¸è¶…è¿‡15é¡µï¼‰
            page_size: æ¯é¡µå¤§å°
            hours_limit: æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰ï¼Œåªæ‹‰å–è¿‡å»Nå°æ—¶çš„æ¨æ–‡ï¼Œé»˜è®¤2å°æ—¶
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        self.crawl_count += 1
        self.last_crawl_time = datetime.now()
        
        try:
            self.logger.info(f"å¼€å§‹çˆ¬å–é¡¹ç›®æ¨æ–‡æ•°æ® (ç¬¬ {self.crawl_count} æ¬¡ï¼Œæ—¶é—´é™åˆ¶: {hours_limit}å°æ—¶)")
            
            # 1. ä»APIè·å–æ•°æ®ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„list_ids_projectï¼‰
            list_ids_project = config.get('api.default_params.list_ids_project', [])
            if not list_ids_project:
                self.logger.error("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°list_ids_project")
                self.error_count += 1
                return False
            
            self.logger.info(f"ä½¿ç”¨é¡¹ç›®åˆ—è¡¨IDs: {list_ids_project}")
            
            api_data_list = self._fetch_api_data(None, list_ids_project, max_pages, page_size, hours_limit)
            
            if not api_data_list:
                self.logger.warning("æœªè·å–åˆ°ä»»ä½•é¡¹ç›®æ¨æ–‡APIæ•°æ®")
                self.error_count += 1
                return False
            
            self.logger.info(f"ä»APIè·å–åˆ° {len(api_data_list)} æ¡é¡¹ç›®æ¨æ–‡åŸå§‹æ•°æ®")
            
            # 2. æ•°æ®æ˜ å°„å’Œè½¬æ¢ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            tweets = self._map_data_to_tweets(api_data_list)
            
            if not tweets:
                self.logger.warning("é¡¹ç›®æ¨æ–‡æ•°æ®æ˜ å°„åæ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®")
                self.error_count += 1
                return False
            
            self.logger.info(f"æˆåŠŸæ˜ å°„ {len(tweets)} æ¡é¡¹ç›®æ¨æ–‡æ•°æ®")
            
            # 2.1 æå–ç”¨æˆ·æ•°æ®
            users = self._extract_users_from_api_data(api_data_list)
            self.logger.info(f"æˆåŠŸæå– {len(users)} æ¡ç”¨æˆ·æ•°æ®")
            
            # 2.2 æ„å»ºç”¨æˆ·æ•°æ®æ˜ å°„ï¼ˆç”¨äºæ¨æ–‡å¢å¼ºï¼‰
            user_data_map = {}
            for api_data in api_data_list:
                try:
                    tweet_id = api_data.get('id_str')
                    # å…¼å®¹ 'user' å’Œ 'author' ä¸¤ç§å­—æ®µï¼ˆæ–°æ¥å£ä½¿ç”¨ authorï¼‰
                    user_data = api_data.get('user') or api_data.get('author')
                    if tweet_id and user_data and isinstance(user_data, dict):
                        user_data_map[tweet_id] = user_data
                except Exception as e:
                    self.logger.warning(f"æ„å»ºç”¨æˆ·æ•°æ®æ˜ å°„å¤±è´¥: {e}")
                    continue
            
            # 2.3 é¡¹ç›®æ¨æ–‡æç®€åŒ–å¢å¼ºï¼ˆä½¿ç”¨ä¸“é—¨çš„ç®€åŒ–å¢å¼ºå™¨ï¼Œæ— å¤æ‚æ•°æ®åŠ è½½ï¼‰
            self.logger.info("å¼€å§‹é¡¹ç›®æ¨æ–‡æç®€åŒ–å¢å¼º...")
            enriched_tweets = simple_tweet_enricher.enrich_project_tweets_simple(tweets, user_data_map)
            self.logger.info(f"é¡¹ç›®æ¨æ–‡æç®€åŒ–å¢å¼ºå®Œæˆï¼Œå¤„ç†äº† {len(enriched_tweets)} æ¡æ¨æ–‡")
            
            # 3. ç®€åŒ–å­˜å‚¨åˆ°æ•°æ®åº“ï¼ˆè·³è¿‡å¤æ‚çš„å¤„ç†é€»è¾‘ï¼‰
            # å…ˆä¿å­˜ç”¨æˆ·æ•°æ®
            if users:
                user_saved_count = self._save_users_to_database(users)
                self.logger.info(f"æˆåŠŸä¿å­˜ {user_saved_count} æ¡ç”¨æˆ·æ•°æ®")
            
            # ä¿å­˜é¡¹ç›®æ¨æ–‡æ•°æ®ï¼ˆä½¿ç”¨æ™®é€šæ¨æ–‡çš„å…¥åº“é€»è¾‘ï¼‰
            tweet_saved_count = self._save_tweets_to_database(enriched_tweets)
            
            # 4. æ•°æ®ä¿å­˜å®Œæˆ
            if tweet_saved_count > 0:
                self.logger.info(f"æˆåŠŸä¿å­˜ {tweet_saved_count} æ¡é¡¹ç›®æ¨æ–‡åˆ°æ•°æ®åº“")
                self.success_count += 1

                # æ‰“å° API è°ƒç”¨ç»Ÿè®¡
                api_stats = self.api_client.get_request_stats()
                self.logger.info("=" * 50)
                self.logger.info("ğŸ“Š API è°ƒç”¨ç»Ÿè®¡")
                self.logger.info("=" * 50)
                self.logger.info(f"æ€»è¯·æ±‚æ¬¡æ•°: {api_stats.get('total_requests', 0)}")
                self.logger.info(f"è·å–æ¨æ–‡æ•°: {api_stats.get('tweets_fetched', 0)}")
                self.logger.info(f"é”™è¯¯æ¬¡æ•°: {api_stats.get('error_count', 0)}")
                self.logger.info(f"æˆåŠŸç‡: {api_stats.get('success_rate', 0):.2f}%")
                if api_stats.get('total_requests', 0) > 0:
                    self.logger.info(f"å¹³å‡æ¯æ¬¡è¯·æ±‚è·å–æ¨æ–‡æ•°: {api_stats.get('avg_tweets_per_request', 0):.1f}")
                # æ˜¾ç¤ºæˆæœ¬ä¿¡æ¯
                self.logger.info("=" * 50)
                self.logger.info("ğŸ’° API æˆæœ¬ç»Ÿè®¡")
                self.logger.info("=" * 50)
                self.logger.info(f"æœ¬æ¬¡æ€»æˆæœ¬: ${api_stats.get('total_cost_usd', 0):.6f} USD")
                self.logger.info(f"å¹³å‡æ¯æ¬¡è¯·æ±‚æˆæœ¬: ${api_stats.get('avg_cost_per_request', 0):.6f} USD")
                self.logger.info(f"æ¯æ¡æ¨æ–‡å¹³å‡æˆæœ¬: ${api_stats.get('total_cost_usd', 0) / max(api_stats.get('tweets_fetched', 1), 1):.6f} USD")
                self.logger.info("=" * 50)

                return True
            else:
                self.logger.error("ä¿å­˜é¡¹ç›®æ¨æ–‡åˆ°æ•°æ®åº“å¤±è´¥")
                self.error_count += 1
                return False
                
        except Exception as e:
            self.logger.error(f"çˆ¬å–é¡¹ç›®æ¨æ–‡æ•°æ®å¼‚å¸¸: {e}")
            self.error_count += 1
            return False
    
    def close(self) -> None:
        """å…³é—­çˆ¬è™«ï¼Œæ¸…ç†èµ„æº"""
        try:
            self.api_client.close()
            self.tweet_dao.db_manager.close()
            self.user_dao.db_manager.close()
            self.quotation_dao.db_manager.close()
            self.logger.info("çˆ¬è™«èµ„æºå·²æ¸…ç†")
        except Exception as e:
            self.logger.error(f"æ¸…ç†çˆ¬è™«èµ„æºæ—¶å‡ºé”™: {e}")


# å…¨å±€çˆ¬è™«å®ä¾‹
crawler = TwitterCrawler() 