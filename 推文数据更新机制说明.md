# 推文数据更新机制说明

## 📋 当前运行状态

### 1. 正在运行的服务

根据系统检查，当前有以下服务在运行：

#### ✅ Marco 数据生成服务（Crontab）
```bash
# Crontab配置
*/30 * * * * cd /Users/qmk/Documents/code/twitter-data-product/twitter-crawler && python run_marco.py --quiet --log-file /Users/qmk/Documents/code/twitter-data-product/twitter-crawler/logs/marco_cron.log
0 1 * * * cd /Users/qmk/Documents/code/twitter-data-product/twitter-crawler && python run_marco.py today --quiet --log-file /Users/qmk/Documents/code/twitter-data-product/twitter-crawler/logs/marco_cron.log
```
- **运行频率**: 每30分钟
- **功能**: 生成 Marco 数据
- **状态**: ✅ 正常运行

#### ✅ N8N 工作流服务（推测）
根据 `N8N_SETUP_GUIDE.md` 文档，项目配置了 N8N 工作流：
- **运行频率**: 每30分钟
- **功能**: 抓取 Twitter 推文并存入数据库
- **配置文件**: `n8n-twitter-topics-workflow.json`
- **数据源**: TweetScout API
- **状态**: ✅ 可能正在运行（推文数据正常更新）

#### ❌ 爬虫服务（未运行）
```bash
./start_crawler_service.sh status
# 输出: Service not running
```
- **脚本**: `start_crawler_service.sh`
- **功能**: 爬取推文 + 项目分析
- **状态**: ❌ 未运行

### 2. 外部服务

发现有一个 Python 进程在运行：
```bash
PID: 70812
进程: /opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/Resources/Python.app/Contents/MacOS/Python run.py
工作目录: /Users/qmk/Documents/code/data-api-py/data-api-service
```

这可能是一个 API 服务，可能也在处理推文数据。

## 🔍 推文数据更新途径分析

### 方式1：N8N 工作流（主要途径）⭐

**工作流程**:
1. N8N 定时触发器每30分钟执行
2. 调用 TweetScout API 获取推文
3. 预处理和清洗数据
4. 保存到 `twitter_tweet` 表

**配置文件**: `N8N_SETUP_GUIDE.md`

**优点**:
- 自动化程度高
- 可视化流程管理
- 内置错误处理

**确认方法**:
```bash
# 检查 N8N 服务状态（如果是 Docker 运行）
docker ps | grep n8n

# 或检查 N8N 进程
ps aux | grep n8n
```

### 方式2：爬虫服务（推荐，但未运行）

**启动脚本**: `start_crawler_service.sh`

```bash
# 查看脚本内容
cat start_crawler_service.sh

# 启动服务（会同时执行项目分析）
./start_crawler_service.sh start

# 检查状态
./start_crawler_service.sh status

# 查看日志
./start_crawler_service.sh logs 50
```

**工作流程**:
1. 每5分钟执行一次爬取
2. 调用 TweetScout API 获取推文
3. 增强推文数据（KOL标记、情感分析等）
4. 保存到数据库
5. **自动执行项目分析**（重要！）

**命令**:
```bash
# 命令格式
python main.py --mode schedule --interval 5
```

### 方式3：手动执行

**单次爬取**:
```bash
cd /Users/qmk/Documents/code/twitter-data-product/twitter-crawler

# 使用 run.sh
./run.sh once

# 或直接使用 main.py
python3 main.py --mode once --max-pages 3 --page-size 100
```

### 方式4：其他定时任务

检查是否有其他 crontab 任务：
```bash
# 检查所有 cron 任务
crontab -l

# 检查系统级 cron 任务
sudo crontab -l

# 检查其他用户的 cron
sudo ls -la /var/spool/cron/crontabs/
```

## 🚀 启动脚本汇总

### 1. 爬虫服务启动脚本（包含项目分析）

#### start_crawler_service.sh ⭐⭐⭐
```bash
# 位置
/Users/qmk/Documents/code/twitter-data-product/twitter-crawler/start_crawler_service.sh

# 用法
./start_crawler_service.sh start    # 启动服务
./start_crawler_service.sh stop     # 停止服务
./start_crawler_service.sh restart  # 重启服务
./start_crawler_service.sh status   # 查看状态
./start_crawler_service.sh logs 50  # 查看日志

# 功能
- 每5分钟爬取推文
- 自动执行项目分析
- 防休眠保护（macOS）
- 后台运行
```

#### start_service.sh
```bash
# 位置
/Users/qmk/Documents/code/twitter-data-product/twitter-crawler/start_service.sh

# 用法
./start_service.sh start    # 启动服务
./start_service.sh stop     # 停止服务
./start_service.sh status   # 查看状态
```

#### run.sh
```bash
# 位置
/Users/qmk/Documents/code/twitter-data-product/twitter-crawler/run.sh

# 用法
./run.sh once      # 单次执行
./run.sh schedule  # 定时调度
./run.sh project   # 项目分析
./run.sh topic     # 话题分析
./run.sh kol       # KOL分析
./run.sh test      # 测试连接
```

### 2. Marco 数据生成脚本

#### run_marco.py
```bash
# 位置
/Users/qmk/Documents/code/twitter-data-product/twitter-crawler/run_marco.py

# 用法
python run_marco.py                 # 生成最新数据
python run_marco.py daemon          # 守护进程模式
python run_marco.py timer           # 定时器模式
python run_marco.py schedule 15     # 自定义间隔
python run_marco.py today           # 回填今天
python run_marco.py stats           # 查看统计
```

### 3. 话题分析服务脚本

#### start_topic_service.sh
```bash
# 位置
/Users/qmk/Documents/code/twitter-data-product/twitter-crawler/start_topic_service.sh

# 用法
./start_topic_service.sh start   # 启动话题分析服务
./start_topic_service.sh stop    # 停止服务
./start_topic_service.sh status  # 查看状态
```

## 📊 推荐的启动方案

### 方案A：完整自动化方案（推荐）⭐⭐⭐

**启动爬虫服务（包含项目分析）**:
```bash
cd /Users/qmk/Documents/code/twitter-data-product/twitter-crawler

# 1. 启动爬虫服务（每5分钟爬取+项目分析）
./start_crawler_service.sh start

# 2. 验证服务状态
./start_crawler_service.sh status

# 3. 查看实时日志
tail -f logs/crawler_service.log
```

**特点**:
- ✅ 推文爬取自动化
- ✅ 项目分析自动化
- ✅ 后台持续运行
- ✅ 防休眠保护

### 方案B：分离式方案

**使用 N8N + 独立项目分析**:

1. **推文爬取**: 由 N8N 工作流负责（已在运行）
2. **项目分析**: 添加 crontab 定时任务

```bash
# 编辑 crontab
crontab -e

# 添加以下行（每2小时执行一次项目分析）
0 */2 * * * cd /Users/qmk/Documents/code/twitter-data-product/twitter-crawler && python3 main.py --mode project --max-pages 5 --page-size 10 >> logs/project_analysis_cron.log 2>&1
```

**特点**:
- ✅ 推文和项目分析独立运行
- ✅ 更灵活的频率控制
- ✅ 减少 API 消耗
- ⚠️ 需要手动配置

### 方案C：手动执行方案

**适合测试或低频更新**:
```bash
cd /Users/qmk/Documents/code/twitter-data-product/twitter-crawler

# 每次手动执行
./run.sh project
```

## 🔍 验证推文数据来源

执行以下命令确认推文数据的更新机制：

```bash
# 1. 检查 N8N 服务
docker ps | grep n8n
ps aux | grep n8n

# 2. 检查最近的推文更新时间
cd /Users/qmk/Documents/code/twitter-data-product/twitter-crawler
python3 -c "
from src.database.connection import db_manager
sql = 'SELECT MAX(update_time) as latest, COUNT(*) as count FROM twitter_tweet WHERE update_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)'
result = db_manager.execute_query(sql)
print(f'最近1小时入库推文数: {result[0][\"count\"]}')
print(f'最新更新时间: {result[0][\"latest\"]}')
"

# 3. 检查 N8N 日志（如果可访问）
# 根据你的 N8N 部署方式查看日志
```

## 📝 总结

### 当前情况
1. **推文数据**: ✅ 正常更新（可能通过 N8N）
2. **项目数据**: ❌ 16天未更新（爬虫服务未运行）
3. **Marco数据**: ✅ 正常更新（crontab 任务）

### 需要执行的操作

**立即修复项目数据问题**:
```bash
cd /Users/qmk/Documents/code/twitter-data-product/twitter-crawler

# 方式1：启动完整爬虫服务（推荐）
./start_crawler_service.sh start

# 方式2：立即执行一次项目分析
python3 main.py --mode project --max-pages 5 --page-size 10
```

### 长期运维建议

1. **使用方案A**（完整自动化）或**方案B**（分离式）
2. 定期监控服务状态
3. 设置告警机制
4. 定期备份数据

## 📞 相关文档

- `N8N_SETUP_GUIDE.md` - N8N 工作流配置
- `SERVICE_README.md` - 服务管理说明
- `ANTI_SLEEP_README.md` - 防休眠配置
- `fix_project_data_update.md` - 项目数据修复指南
- `SUMMARY_项目数据停止更新问题分析.md` - 完整分析报告

