#!/usr/bin/env python3
"""
åˆ†ææ—¶åºé—®é¢˜çš„æ ¹æœ¬åŸå› 
"""
import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.database.connection import db_manager
from src.utils.logger import setup_logger


def analyze_timing_issue_root_cause():
    """åˆ†ææ—¶åºé—®é¢˜çš„æ ¹æœ¬åŸå› """
    setup_logger()
    
    print("ğŸ” åˆ†æpopularity_historyæ—¶åºé—®é¢˜æ ¹æœ¬åŸå› ")
    print("=" * 70)
    
    try:
        # æŸ¥çœ‹ä¿®å¤åçš„æ•°æ®åˆ†å¸ƒ
        analysis_sql = """
        SELECT 
            topic_id,
            topic_name,
            created_at,
            update_time,
            popularity_history
        FROM topics
        ORDER BY created_at ASC
        """
        
        topics = db_manager.execute_query(analysis_sql)
        
        print(f"1ï¸âƒ£ æ—¶åºæ¨¡å¼åˆ†æ:")
        
        import json
        from datetime import datetime
        
        early_batch = []  # æ—©ä¸Šåˆ›å»ºçš„
        late_batch = []   # ä¸‹åˆåˆ›å»ºçš„
        
        for topic in topics:
            created_time = topic['created_at']
            update_time = topic['update_time']
            
            # è§£æcreated_atæ—¶é—´
            if isinstance(created_time, datetime):
                hour = created_time.hour
            else:
                try:
                    created_dt = datetime.fromisoformat(str(created_time))
                    hour = created_dt.hour
                except:
                    hour = 12  # é»˜è®¤å€¼
            
            if hour < 12:
                early_batch.append(topic)
            else:
                late_batch.append(topic)
        
        print(f"   æ—©ä¸Šåˆ›å»ºçš„è¯é¢˜: {len(early_batch)} ä¸ª (06:44-07:04)")
        print(f"   ä¸‹åˆåˆ›å»ºçš„è¯é¢˜: {len(late_batch)} ä¸ª (15:38-15:39)")
        
        # åˆ†ææ—¶åºå·®å¼‚
        print(f"\n2ï¸âƒ£ æ—¶åºå·®å¼‚åˆ†æ:")
        
        if early_batch:
            sample_early = early_batch[0]
            print(f"   æ—©æœŸè¯é¢˜ç¤ºä¾‹: {sample_early['topic_name'][:30]}...")
            print(f"     åˆ›å»ºæ—¶é—´: {sample_early['created_at']}")
            print(f"     æ›´æ–°æ—¶é—´: {sample_early['update_time']}")
            
            time_diff = None
            if (sample_early['update_time'] and sample_early['created_at']):
                try:
                    if isinstance(sample_early['update_time'], datetime) and isinstance(sample_early['created_at'], datetime):
                        time_diff = sample_early['update_time'] - sample_early['created_at']
                    print(f"     æ—¶é—´å·®: {time_diff}")
                except:
                    print(f"     æ—¶é—´å·®: æ— æ³•è®¡ç®—")
        
        if late_batch:
            sample_late = late_batch[0]
            print(f"   æ™šæœŸè¯é¢˜ç¤ºä¾‹: {sample_late['topic_name'][:30]}...")
            print(f"     åˆ›å»ºæ—¶é—´: {sample_late['created_at']}")
            print(f"     æ›´æ–°æ—¶é—´: {sample_late['update_time']}")
        
        # 3. åˆ†ææ ¹æœ¬åŸå› 
        print(f"\n3ï¸âƒ£ æ ¹æœ¬åŸå› åˆ†æ:")
        
        print("   å‘ç°çš„æ—¶åºé—®é¢˜:")
        print("   1. æ—©æœŸè¯é¢˜: åˆ›å»ºæ—¶é—´å’Œæ›´æ–°æ—¶é—´ä¸åŒ (æœ‰8å°æ—¶æ—¶å·®)")
        print("   2. æ™šæœŸè¯é¢˜: åˆ›å»ºæ—¶é—´å’Œæ›´æ–°æ—¶é—´ç›¸åŒ")
        print("   3. è¿™è¯´æ˜å­˜åœ¨ä¸¤ç§ä¸åŒçš„è¯é¢˜åˆ›å»ºè·¯å¾„")
        
        print(f"\n   å¯èƒ½çš„åŸå› :")
        print("   A. æ—©æœŸè¯é¢˜é€šè¿‡ä¸€ç§æµç¨‹åˆ›å»ºï¼Œåæ¥è¢«å¦ä¸€ä¸ªæµç¨‹æ›´æ–°")
        print("   B. æ™šæœŸè¯é¢˜é€šè¿‡æ–°çš„æµç¨‹ç›´æ¥åˆ›å»ºï¼Œæ²¡æœ‰åç»­æ›´æ–°")
        print("   C. add_popularity_history()è°ƒç”¨æ—¶æœºä¸åŒ")
        
        # 4. æŸ¥çœ‹ç›¸å…³æ¨æ–‡çš„åˆ›å»ºæ—¶é—´
        print(f"\n4ï¸âƒ£ ç›¸å…³æ¨æ–‡åˆ›å»ºæ—¶é—´åˆ†æ:")
        
        tweet_time_sql = """
        SELECT 
            topic_id,
            COUNT(*) as tweet_count,
            MIN(created_at_datetime) as earliest_tweet,
            MAX(created_at_datetime) as latest_tweet
        FROM twitter_tweet
        WHERE topic_id IS NOT NULL
        GROUP BY topic_id
        ORDER BY earliest_tweet ASC
        LIMIT 5
        """
        
        tweet_times = db_manager.execute_query(tweet_time_sql)
        
        for tweet_info in tweet_times:
            topic_id = tweet_info['topic_id']
            # æ‰¾åˆ°å¯¹åº”çš„è¯é¢˜
            matching_topic = next((t for t in topics if t['topic_id'] == topic_id), None)
            if matching_topic:
                print(f"   {matching_topic['topic_name'][:25]}...")
                print(f"     è¯é¢˜åˆ›å»º: {matching_topic['created_at']}")
                print(f"     æ¨æ–‡èŒƒå›´: {tweet_info['earliest_tweet']} ~ {tweet_info['latest_tweet']}")
                print(f"     æ¨æ–‡æ•°é‡: {tweet_info['tweet_count']}")
        
        return {
            'early_batch_count': len(early_batch),
            'late_batch_count': len(late_batch),
            'total_topics': len(topics)
        }
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == '__main__':
    result = analyze_timing_issue_root_cause()
    if result:
        print("=" * 70)
        print("ğŸ¯ æ ¹æœ¬åŸå› æ€»ç»“")
        print("=" * 70)
        print("æ—¶åºé—®é¢˜çš„æ ¹æœ¬åŸå› :")
        print("1. è¯é¢˜åˆ›å»ºæµç¨‹å­˜åœ¨ä¸¤ç§è·¯å¾„")
        print("2. æ—©æœŸè·¯å¾„: åˆ›å»º -> åç»­æ›´æ–° -> æ·»åŠ å†å²è®°å½•")
        print("3. æ™šæœŸè·¯å¾„: ç›´æ¥åˆ›å»º -> æ²¡æœ‰åç»­æ›´æ–° -> ç¼ºå¤±å†å²è®°å½•")
        print("4. add_popularity_history()è°ƒç”¨æ—¶æœºä¸ä¸€è‡´")
    sys.exit(0)