#!/usr/bin/env python3
"""
Twitteræ•°æ®çˆ¬è™«ä¸»ç¨‹åº
"""
import argparse
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.crawler import crawler
from src.utils.scheduler import scheduler
from src.utils.logger import get_logger
from src.utils.config_manager import config
from src.utils.health_monitor import HealthMonitor
# from src.topic_engine import topic_engine  # è¯é¢˜åˆ†æå·²ç§»é™¤ï¼Œåœ¨ç‹¬ç«‹æœåŠ¡ä¸­å¤„ç†
# from src.kol_engine import kol_engine  # KOLåˆ†æå·²ç¦ç”¨
from src.project_engine import project_engine

# åˆå§‹åŒ–å¥åº·ç›‘æ§å™¨
health_monitor = HealthMonitor()


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logger = get_logger(__name__)
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='Twitteræ•°æ®çˆ¬è™«')
    parser.add_argument('--mode', choices=['once', 'schedule', 'test', 'topic', 'project', 'project-once', 'project-schedule', 'health', 'cost'], default='once',
                       help='è¿è¡Œæ¨¡å¼: once=å•æ¬¡æ‰§è¡Œ, schedule=å®šæ—¶è°ƒåº¦, test=æµ‹è¯•è¿æ¥, health=å¥åº·æ£€æŸ¥, cost=æˆæœ¬ç»Ÿè®¡')
    parser.add_argument('--max-pages', type=int, help='æœ€å¤§é¡µæ•°')
    parser.add_argument('--page-size', type=int, help='æ¯é¡µå¤§å°')
    parser.add_argument('--interval', type=int, help='è°ƒåº¦é—´éš”(åˆ†é’Ÿ)')
    parser.add_argument('--hours-limit', type=float, default=3, help='æ—¶é—´é™åˆ¶(å°æ—¶ï¼Œå¯ä¸ºå°æ•°)ï¼Œåªæ‹‰å–è¿‡å»Nå°æ—¶çš„æ¨æ–‡ï¼Œé»˜è®¤3å°æ—¶')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--cost-period', type=int, default=24, help='æˆæœ¬ç»Ÿè®¡å‘¨æœŸ(å°æ—¶)ï¼Œé»˜è®¤24å°æ—¶')
    
    args = parser.parse_args()
    
    logger.info("=" * 50)
    logger.info("Twitteræ•°æ®çˆ¬è™«å¯åŠ¨")
    logger.info("=" * 50)
    
    try:
        # å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼Œé‡æ–°åŠ è½½é…ç½®
        if args.config:
            config.config_file = Path(args.config)
            config.reload_config()
            logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        
        # æ ¹æ®æ¨¡å¼æ‰§è¡Œç›¸åº”æ“ä½œ
        if args.mode == 'test':
            run_tests()
        elif args.mode == 'once':
            run_once(args)
        elif args.mode == 'schedule':
            run_scheduled(args)
        elif args.mode == 'topic':
            run_topic_analysis(args)
        # elif args.mode == 'kol':  # KOLåˆ†æå·²ç¦ç”¨
        #     run_kol_analysis(args)
        elif args.mode == 'project':
            run_project_analysis(args)
        elif args.mode == 'project-once':
            run_project_once(args)
        elif args.mode == 'project-schedule':
            run_project_scheduled(args)
        elif args.mode == 'health':
            run_health_check()
        elif args.mode == 'cost':
            run_cost_stats(args)
        
    except KeyboardInterrupt:
        logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1)
    finally:
        # æ¸…ç†èµ„æº
        cleanup()


def run_tests():
    """è¿è¡Œè¿æ¥æµ‹è¯•"""
    logger = get_logger(__name__)
    
    logger.info("å¼€å§‹è¿è¡Œè¿æ¥æµ‹è¯•...")
    
    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    db_success = crawler.test_connection()
    
    # æµ‹è¯•APIè¿æ¥
    api_success = crawler.test_api_connection()
    
    # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
    logger.info("=" * 30)
    logger.info("æµ‹è¯•ç»“æœ:")
    logger.info(f"æ•°æ®åº“è¿æ¥: {'âœ“ æˆåŠŸ' if db_success else 'âœ— å¤±è´¥'}")
    logger.info(f"APIè¿æ¥: {'âœ“ æˆåŠŸ' if api_success else 'âœ— å¤±è´¥'}")
    logger.info("=" * 30)
    
    if db_success and api_success:
        logger.info("æ‰€æœ‰è¿æ¥æµ‹è¯•é€šè¿‡")
        sys.exit(0)
    else:
        logger.error("è¿æ¥æµ‹è¯•å¤±è´¥")
        sys.exit(1)


def run_once(args):
    """å•æ¬¡æ‰§è¡Œçˆ¬å–"""
    logger = get_logger(__name__)

    logger.info("å¼€å§‹å•æ¬¡æ•°æ®çˆ¬å–...")

    # å…ˆè¿›è¡Œå¥åº·æ£€æŸ¥
    health_status = health_monitor.check_health()
    if health_status['should_stop']:
        logger.error("å¥åº·æ£€æŸ¥å¤±è´¥ï¼ŒæœåŠ¡åº”è¯¥åœæ­¢ï¼")
        for alert in health_status['alerts']:
            logger.error(alert)
        sys.exit(1)

    # æ˜¾ç¤ºè­¦å‘Šï¼ˆå¦‚æœæœ‰ï¼‰
    for warning in health_status['warnings']:
        logger.warning(warning)

    # æ‰§è¡Œçˆ¬å–ï¼ˆç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„list_idsè¿›è¡Œå¹¶è¡Œè·å–ï¼Œä½¿ç”¨æ™ºèƒ½æ—¶é—´æ£€æµ‹ï¼‰
    success = crawler.crawl_tweets(
        max_pages=args.max_pages,
        page_size=args.page_size,
        hours_limit=args.hours_limit
    )

    # è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯
    stats = crawler.get_statistics()
    logger.info("=" * 30)
    logger.info("çˆ¬å–ç»Ÿè®¡:")
    for key, value in stats.items():
        logger.info(f"{key}: {value}")
    logger.info("=" * 30)

    # è®°å½•ç»Ÿè®¡åˆ°å¥åº·ç›‘æ§ï¼ˆéœ€è¦ä»crawlerè·å–è¯¦ç»†ç»Ÿè®¡ï¼‰
    try:
        from datetime import datetime
        # è®¡ç®—æœ‰æ•ˆ/æ— æ•ˆæ¨æ–‡æ•°ï¼ˆä»æ•°æ®åº“æœ€è¿‘è®°å½•ä¸­ç»Ÿè®¡ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä»crawlerè¿”å›çš„è¯¦ç»†ç»Ÿè®¡ä¸­è·å–
        health_monitor.record_crawl_stats({
            'timestamp': datetime.now().isoformat(),
            'total_tweets': stats.get('database_tweet_count', 0),  # ç®€åŒ–ï¼šä½¿ç”¨æ€»æ•°
            'valid_tweets': int(stats.get('database_tweet_count', 0) * 0.95),  # ä¼°ç®—ï¼Œåº”ä»crawlerè·å–
            'invalid_tweets': int(stats.get('database_tweet_count', 0) * 0.05),
            'twitter_api_cost': stats.get('api_stats', {}).get('total_cost_usd', 0),
            'ai_api_cost': 0,  # TODO: ä»AIè°ƒç”¨ä¸­è·å–
            'total_cost': stats.get('api_stats', {}).get('total_cost_usd', 0),
        })
    except Exception as e:
        logger.warning(f"è®°å½•å¥åº·ç»Ÿè®¡å¤±è´¥: {e}")

    if success:
        logger.info("æ•°æ®çˆ¬å–å®Œæˆ")
        logger.info("å•æ¬¡æ‰§è¡Œå®Œæˆ")
        sys.exit(0)
    else:
        logger.error("æ•°æ®çˆ¬å–å¤±è´¥")
        sys.exit(1)


def run_scheduled(args):
    """å®šæ—¶è°ƒåº¦æ‰§è¡Œ"""
    logger = get_logger(__name__)
    
    logger.info("å¼€å§‹å®šæ—¶è°ƒåº¦æ¨¡å¼...")
    
    # è®¾ç½®è°ƒåº¦é—´éš”
    if args.interval:
        scheduler.update_interval(args.interval)
    
    # åˆ›å»ºçˆ¬å–ä»»åŠ¡å‡½æ•°
    def crawl_task():
        """å®šæ—¶çˆ¬å–ä»»åŠ¡ï¼ˆåŒ…å«é¡¹ç›®åˆ†æï¼‰"""
        logger.info("æ‰§è¡Œå®šæ—¶çˆ¬å–ä»»åŠ¡...")
        
        # æ‰§è¡Œçˆ¬å–ï¼ˆç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„list_idsè¿›è¡Œå¹¶è¡Œè·å–ï¼Œä½¿ç”¨æ™ºèƒ½æ—¶é—´æ£€æµ‹ï¼‰
        crawl_success = crawler.crawl_tweets(
            max_pages=args.max_pages,
            page_size=args.page_size,
            hours_limit=args.hours_limit
        )
        
        if crawl_success:
            logger.info("çˆ¬å–å®Œæˆï¼Œå¼€å§‹é¡¹ç›®åˆ†æ...")
            # æ‰§è¡Œé¡¹ç›®åˆ†æï¼ˆé™åˆ¶æ¨æ–‡æ•°é‡ä»¥æé«˜é€Ÿåº¦ï¼‰
            max_tweets = min(50, (args.max_pages or 3) * (args.page_size or 100))
            project_success = project_engine.analyze_recent_tweets(hours=24, max_tweets=max_tweets)
            
            if project_success:
                logger.info("é¡¹ç›®åˆ†æå®Œæˆ")
            else:
                logger.warning("é¡¹ç›®åˆ†æå¤±è´¥")
        
        return crawl_success
    
    # è®¾ç½®è°ƒåº¦å™¨
    scheduler.set_crawler(crawl_task)
    
    # å…ˆæ‰§è¡Œä¸€æ¬¡æµ‹è¯•
    logger.info("æ‰§è¡Œè¿æ¥æµ‹è¯•...")
    if not (crawler.test_connection() and crawler.test_api_connection()):
        logger.error("è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨å®šæ—¶è°ƒåº¦")
        sys.exit(1)
    
    # å¯åŠ¨å®šæ—¶è°ƒåº¦
    scheduler.start_crawling()
    
    logger.info("å®šæ—¶è°ƒåº¦å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢")
    
    try:
        # ä¸»å¾ªç¯ï¼Œå®šæœŸæ˜¾ç¤ºçŠ¶æ€
        while True:
            time.sleep(300)  # æ¯5åˆ†é’Ÿæ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€
            
            # æ˜¾ç¤ºè°ƒåº¦å™¨çŠ¶æ€
            scheduler_status = scheduler.get_status()
            logger.info(f"è°ƒåº¦å™¨çŠ¶æ€: è¿è¡Œä¸­={scheduler_status['is_running']}, "
                       f"ä»»åŠ¡æ•°={scheduler_status['task_count']}, "
                       f"æˆåŠŸç‡={scheduler_status['success_rate']:.1f}%")
            
            # æ˜¾ç¤ºçˆ¬è™«ç»Ÿè®¡
            crawler_stats = crawler.get_statistics()
            logger.info(f"çˆ¬è™«ç»Ÿè®¡: æˆåŠŸ={crawler_stats['success_count']}, "
                       f"å¤±è´¥={crawler_stats['error_count']}, "
                       f"æ•°æ®åº“æ¨æ–‡æ•°={crawler_stats['database_tweet_count']}")
    
    except KeyboardInterrupt:
        logger.info("æ¥æ”¶åˆ°åœæ­¢ä¿¡å·...")
        scheduler.stop()


def run_topic_analysis(args):
    """è¿è¡Œè¯é¢˜åˆ†æ - å·²ç§»é™¤ï¼Œè¯·ä½¿ç”¨ç‹¬ç«‹çš„è¯é¢˜åˆ†æè„šæœ¬"""
    logger = get_logger(__name__)
    
    logger.warning("è¯é¢˜åˆ†æåŠŸèƒ½å·²ä»ä¸»æœåŠ¡ä¸­ç§»é™¤")
    logger.info("è¯·ä½¿ç”¨ä»¥ä¸‹ç‹¬ç«‹æœåŠ¡:")
    logger.info("- ./start_topic_service.sh start  # ç‹¬ç«‹çš„è¯é¢˜åˆ†ææœåŠ¡")
    logger.info("- python main.py --mode topic     # ä»…åœ¨éœ€è¦æ—¶æ‰‹åŠ¨è¿è¡Œ")
    
    # åŠŸèƒ½å·²ç§»é™¤
    # logger.info("å¼€å§‹è¯é¢˜åˆ†ææ¨¡å¼...")
    # 
    # # åˆ†æç°æœ‰æ¨æ–‡æ•°æ®
    # max_tweets = args.max_pages * (args.page_size or 10) if args.max_pages and args.page_size else 20
    # 
    # success = topic_engine.analyze_recent_tweets(hours=24, max_tweets=max_tweets)
    # 
    # # æ˜¾ç¤ºåˆ†æç»“æœ
    # stats = topic_engine.get_topic_statistics()
    # logger.info("=" * 30)
    # logger.info("è¯é¢˜åˆ†æç»Ÿè®¡:")
    # for key, value in stats.items():
    #     if key != 'hot_topics_sample':  # è·³è¿‡å¤æ‚çš„åµŒå¥—æ•°æ®
    #         logger.info(f"{key}: {value}")
    # 
    # # æ˜¾ç¤ºç”Ÿæˆçš„è¯é¢˜
    # from src.database.topic_dao import topic_dao
    # topic_count = topic_dao.get_topic_count()
    # if topic_count > 0:
    #     hot_topics = topic_dao.get_hot_topics(limit=3)
    #     logger.info("\næœ€æ–°çƒ­é—¨è¯é¢˜:")
    #     for i, topic in enumerate(hot_topics, 1):
    #         logger.info(f"{i}. {topic.topic_name} (çƒ­åº¦: {topic.popularity})")
    # 
    # logger.info("=" * 30)
    # 
    # if success:
    #     logger.info("è¯é¢˜åˆ†æå®Œæˆ")
    #     sys.exit(0)
    # else:
    #     logger.error("è¯é¢˜åˆ†æå¤±è´¥")
    #     sys.exit(1)
    
    sys.exit(0)


# KOLåˆ†æåŠŸèƒ½å·²ç¦ç”¨
# def run_kol_analysis(args):
#     """è¿è¡ŒKOLåˆ†æ"""
#     logger = get_logger(__name__)
#     logger.warning("KOLåˆ†æåŠŸèƒ½å·²ç¦ç”¨")
#     sys.exit(0)


def run_project_analysis(args):
    """è¿è¡Œé¡¹ç›®åˆ†æ"""
    logger = get_logger(__name__)
    
    logger.info("å¼€å§‹é¡¹ç›®åˆ†ææ¨¡å¼...")
    
    # åˆ†æå‚æ•°
    hours = 24  # åˆ†ææœ€è¿‘24å°æ—¶
    max_tweets = args.max_pages * (args.page_size or 10) if args.max_pages and args.page_size else 50
    
    success = project_engine.analyze_recent_tweets(hours=hours, max_tweets=max_tweets)
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    stats = project_engine.get_project_statistics()
    logger.info("=" * 30)
    logger.info("é¡¹ç›®åˆ†æç»Ÿè®¡:")
    for key, value in stats.items():
        if key not in ['category_distribution', 'sentiment_distribution', 'hot_projects', 'chatgpt_stats']:
            logger.info(f"{key}: {value}")
    
    # æ˜¾ç¤ºçƒ­é—¨é¡¹ç›®
    hot_projects = stats.get('hot_projects', [])
    if hot_projects:
        logger.info("\nçƒ­é—¨é¡¹ç›®:")
        for i, project in enumerate(hot_projects, 1):
            logger.info(f"{i}. {project['name']} ({project['symbol']}) - çƒ­åº¦: {project['popularity']}, æƒ…æ„Ÿ: {project['sentiment']}")
    
    # æ˜¾ç¤ºåˆ†ç±»åˆ†å¸ƒ
    category_dist = stats.get('category_distribution', {})
    if category_dist:
        logger.info("\né¡¹ç›®åˆ†ç±»åˆ†å¸ƒ:")
        for category, count in category_dist.items():
            if count > 0:
                logger.info(f"  {category}: {count} ä¸ªé¡¹ç›®")
    
    logger.info("=" * 30)
    
    if success:
        logger.info("é¡¹ç›®åˆ†æå®Œæˆ")
        sys.exit(0)
    else:
        logger.error("é¡¹ç›®åˆ†æå¤±è´¥")
        sys.exit(1)


def run_project_once(args):
    """å•æ¬¡æ‰§è¡Œé¡¹ç›®æ¨æ–‡çˆ¬å–"""
    logger = get_logger(__name__)

    logger.info("å¼€å§‹å•æ¬¡é¡¹ç›®æ¨æ–‡æ•°æ®çˆ¬å–...")

    # è®¾ç½®ä½¿ç”¨é¡¹ç›®æ¨æ–‡ä¸“ç”¨è¡¨
    from src.database.tweet_dao import tweet_dao
    tweet_dao.table_name = 'twitter_tweet_back_test_cmc300'
    logger.info(f"ä½¿ç”¨æ•°æ®è¡¨: {tweet_dao.table_name}")

    # æ‰§è¡Œçˆ¬å–ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„list_ids_projectè¿›è¡Œå¹¶è¡Œè·å–ï¼Œä½¿ç”¨æ™ºèƒ½æ—¶é—´æ£€æµ‹ï¼‰
    success = crawler.crawl_project_tweets(
        max_pages=args.max_pages,
        page_size=args.page_size,
        hours_limit=args.hours_limit
    )

    # æ˜¾ç¤ºçˆ¬å–ç»Ÿè®¡ä¿¡æ¯
    stats = crawler.get_statistics()
    logger.info("=" * 30)
    logger.info("é¡¹ç›®æ¨æ–‡çˆ¬å–ç»Ÿè®¡:")
    for key, value in stats.items():
        logger.info(f"{key}: {value}")
    logger.info("=" * 30)

    if success:
        logger.info("é¡¹ç›®æ¨æ–‡æ•°æ®çˆ¬å–å®Œæˆ")
        logger.info("å•æ¬¡æ‰§è¡Œå®Œæˆ")
        sys.exit(0)
    else:
        logger.error("é¡¹ç›®æ¨æ–‡æ•°æ®çˆ¬å–å¤±è´¥")
        sys.exit(1)


def run_project_scheduled(args):
    """å®šæ—¶è°ƒåº¦æ‰§è¡Œé¡¹ç›®æ¨æ–‡çˆ¬å–"""
    logger = get_logger(__name__)

    logger.info("å¼€å§‹é¡¹ç›®æ¨æ–‡å®šæ—¶è°ƒåº¦æ¨¡å¼...")

    # è®¾ç½®ä½¿ç”¨é¡¹ç›®æ¨æ–‡ä¸“ç”¨è¡¨
    from src.database.tweet_dao import tweet_dao
    tweet_dao.table_name = 'twitter_tweet_back_test_cmc300'
    logger.info(f"ä½¿ç”¨æ•°æ®è¡¨: {tweet_dao.table_name}")

    # è®¾ç½®è°ƒåº¦é—´éš”
    if args.interval:
        scheduler.update_interval(args.interval)
    
    # åˆ›å»ºçˆ¬å–ä»»åŠ¡å‡½æ•°
    def crawl_project_task():
        """å®šæ—¶é¡¹ç›®æ¨æ–‡çˆ¬å–ä»»åŠ¡"""
        logger.info("æ‰§è¡Œå®šæ—¶é¡¹ç›®æ¨æ–‡çˆ¬å–ä»»åŠ¡...")
        
        # æ‰§è¡Œçˆ¬å–ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„list_ids_projectè¿›è¡Œå¹¶è¡Œè·å–ï¼Œä½¿ç”¨æ™ºèƒ½æ—¶é—´æ£€æµ‹ï¼‰
        crawl_success = crawler.crawl_project_tweets(
            max_pages=args.max_pages,
            page_size=args.page_size,
            hours_limit=args.hours_limit
        )
        
        if crawl_success:
            logger.info("é¡¹ç›®æ¨æ–‡çˆ¬å–å®Œæˆ")
        else:
            logger.warning("é¡¹ç›®æ¨æ–‡çˆ¬å–å¤±è´¥")
        
        return crawl_success
    
    # è®¾ç½®è°ƒåº¦å™¨
    scheduler.set_crawler(crawl_project_task)
    
    # å…ˆæ‰§è¡Œä¸€æ¬¡æµ‹è¯•
    logger.info("æ‰§è¡Œè¿æ¥æµ‹è¯•...")
    if not (crawler.test_connection() and crawler.test_api_connection()):
        logger.error("è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨å®šæ—¶è°ƒåº¦")
        sys.exit(1)
    
    # å¯åŠ¨å®šæ—¶è°ƒåº¦
    scheduler.start_crawling()
    
    logger.info("é¡¹ç›®æ¨æ–‡å®šæ—¶è°ƒåº¦å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢")
    
    try:
        # ä¸»å¾ªç¯ï¼Œå®šæœŸæ˜¾ç¤ºçŠ¶æ€
        while True:
            time.sleep(300)  # æ¯5åˆ†é’Ÿæ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€
            
            # æ˜¾ç¤ºè°ƒåº¦å™¨çŠ¶æ€
            scheduler_status = scheduler.get_status()
            logger.info(f"è°ƒåº¦å™¨çŠ¶æ€: è¿è¡Œä¸­={scheduler_status['is_running']}, "
                       f"ä»»åŠ¡æ•°={scheduler_status['task_count']}, "
                       f"æˆåŠŸç‡={scheduler_status['success_rate']:.1f}%")
            
            # æ˜¾ç¤ºçˆ¬è™«ç»Ÿè®¡
            crawler_stats = crawler.get_statistics()
            logger.info(f"çˆ¬è™«ç»Ÿè®¡: æˆåŠŸ={crawler_stats['success_count']}, "
                       f"å¤±è´¥={crawler_stats['error_count']}, "
                       f"æ•°æ®åº“æ¨æ–‡æ•°={crawler_stats['database_tweet_count']}")
    
    except KeyboardInterrupt:
        logger.info("æ¥æ”¶åˆ°åœæ­¢ä¿¡å·...")
        scheduler.stop()


def run_health_check():
    """è¿è¡Œå¥åº·æ£€æŸ¥"""
    logger = get_logger(__name__)

    logger.info("=" * 60)
    logger.info("ğŸ¥ çˆ¬è™«å¥åº·æ£€æŸ¥")
    logger.info("=" * 60)

    # æ‰§è¡Œå¥åº·æ£€æŸ¥
    health_status = health_monitor.check_health()

    # æ˜¾ç¤ºå¥åº·çŠ¶æ€
    if health_status['is_healthy']:
        logger.info("âœ… çŠ¶æ€: å¥åº·")
    else:
        logger.warning("âš ï¸  çŠ¶æ€: å¼‚å¸¸")

    # æ˜¾ç¤ºæŠ¥è­¦
    if health_status['alerts']:
        logger.info("\nğŸš¨ æŠ¥è­¦:")
        for alert in health_status['alerts']:
            logger.error(f"  {alert}")

    # æ˜¾ç¤ºè­¦å‘Š
    if health_status['warnings']:
        logger.info("\nâš ï¸  è­¦å‘Š:")
        for warning in health_status['warnings']:
            logger.warning(f"  {warning}")

    # æ˜¾ç¤ºæˆæœ¬æ‘˜è¦ï¼ˆæœ€è¿‘24å°æ—¶ï¼‰
    cost_summary = health_monitor.get_cost_summary(hours=24)
    logger.info("\nğŸ’° æˆæœ¬æ‘˜è¦ï¼ˆæœ€è¿‘24å°æ—¶ï¼‰:")
    logger.info(f"  Twitter API: ${cost_summary.get('twitter_api_cost', 0):.4f}")
    logger.info(f"  AI API: ${cost_summary.get('ai_api_cost', 0):.4f}")
    logger.info(f"  æ€»è®¡: ${cost_summary.get('total_cost', 0):.4f}")
    logger.info(f"  è¯·æ±‚æ¬¡æ•°: {cost_summary.get('request_count', 0)}")

    logger.info("\n" + "=" * 60)

    # å¦‚æœéœ€è¦åœæ­¢æœåŠ¡ï¼Œé€€å‡ºç ä¸º1
    sys.exit(1 if health_status['should_stop'] else 0)


def run_cost_stats(args):
    """è¿è¡Œæˆæœ¬ç»Ÿè®¡"""
    logger = get_logger(__name__)

    period_hours = args.cost_period

    logger.info("=" * 60)
    logger.info(f"ğŸ’° APIæˆæœ¬ç»Ÿè®¡ï¼ˆæœ€è¿‘{period_hours}å°æ—¶ï¼‰")
    logger.info("=" * 60)

    # è·å–æˆæœ¬æ‘˜è¦
    cost_summary = health_monitor.get_cost_summary(hours=period_hours)

    if cost_summary.get('request_count', 0) == 0:
        logger.info("\nâš ï¸  æš‚æ— ç»Ÿè®¡æ•°æ®")
    else:
        logger.info(f"\nğŸ“Š ç»Ÿè®¡å‘¨æœŸ: æœ€è¿‘ {period_hours} å°æ—¶")
        logger.info(f"ğŸ“ˆ è¯·æ±‚æ¬¡æ•°: {cost_summary.get('request_count', 0)} æ¬¡")
        logger.info(f"\nğŸ’µ æˆæœ¬æ˜ç»†:")
        logger.info(f"  â€¢ Twitter API: ${cost_summary.get('twitter_api_cost', 0):.4f}")
        logger.info(f"  â€¢ AI API: ${cost_summary.get('ai_api_cost', 0):.4f}")
        logger.info(f"  â€¢ æ€»è®¡: ${cost_summary.get('total_cost', 0):.4f}")
        logger.info(f"\nğŸ“‰ å¹³å‡æˆæœ¬:")
        logger.info(f"  â€¢ æ¯æ¬¡è¯·æ±‚: ${cost_summary.get('avg_cost_per_request', 0):.4f}")

        # é¢„ä¼°æ¯æ—¥/æ¯æœˆæˆæœ¬
        if period_hours > 0:
            daily_estimate = cost_summary.get('total_cost', 0) * (24 / period_hours)
            monthly_estimate = daily_estimate * 30
            logger.info(f"\nğŸ“… æˆæœ¬é¢„ä¼°:")
            logger.info(f"  â€¢ æ¯æ—¥: ${daily_estimate:.2f}")
            logger.info(f"  â€¢ æ¯æœˆ: ${monthly_estimate:.2f}")

    logger.info("\n" + "=" * 60)

    sys.exit(0)


def cleanup():
    """æ¸…ç†èµ„æº"""
    logger = get_logger(__name__)
    
    try:
        # åœæ­¢è°ƒåº¦å™¨
        if scheduler.is_running:
            scheduler.stop()
        
        # å…³é—­çˆ¬è™«
        crawler.close()
        
        logger.info("èµ„æºæ¸…ç†å®Œæˆ")
        
    except Exception as e:
        logger.error(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")


if __name__ == '__main__':
    main() 